# ğŸš€ FonctionnalitÃ© de Scrapping - KrosmozJDR

## ğŸ¯ Objectif

Cette fonctionnalitÃ© permet de rÃ©cupÃ©rer, convertir et intÃ©grer des donnÃ©es depuis des sites externes (comme DofusDB) vers KrosmozJDR. Les donnÃ©es qui sont rÃ©cupÃ©rÃ©es depuis des sites externes ne sont pas structurÃ©es comme ce dont a besoin KrosmozJDR. De plus, ces sites utilisent les donnÃ©es du jeu Dofus alors que KrosmozJDR se base sur le jeu Dofus mais avec des donnÃ©es simplifiÃ©es. Il y a donc besoin de convertir.

## ğŸ—ï¸ Architecture

Cette fonctionnalitÃ© de scrapping est composÃ©e de **quatre composants** qui travaillent ensemble :

### **1. DataCollect** ğŸ“¥
- **Objectif** : RÃ©cupÃ©rer les donnÃ©es brutes depuis des sites externes (DofusDB, etc.)
- **ResponsabilitÃ©s** :
  - Collecte des donnÃ©es via API REST
  - Gestion du cache et du rate limiting
  - Gestion des erreurs et retry
  - Filtrage des catÃ©gories d'objets
- **EntitÃ©s collectÃ©es** : Classes, monstres, objets, sorts, effets, ensembles d'objets

### **2. DataConversion** ğŸ”„
- **Objectif** : Convertir les donnÃ©es selon les caractÃ©ristiques et formules KrosmozJDR
- **ResponsabilitÃ©s** :
  - Conversion des valeurs selon les seuils dÃ©finis
  - Application des formules de calcul
  - Validation des donnÃ©es converties
  - Corrections automatiques si nÃ©cessaire
- **CaractÃ©ristiques** : Service **agnostique** Ã  la source de donnÃ©es

### **3. DataIntegration** ğŸ”—
- **Objectif** : Ã‰tablir le lien entre les donnÃ©es converties et la structure KrosmozJDR
- **ResponsabilitÃ©s** :
  - Mapping structurel entre DofusDB et KrosmozJDR
  - Gestion des relations entre entitÃ©s
  - Sauvegarde en base de donnÃ©es
  - Gestion des conflits et doublons
- **CaractÃ©ristiques** : GÃ¨re le **mapping structurel** spÃ©cifique Ã  DofusDB

### **4. ScrappingOrchestrator** ğŸ¼
- **Objectif** : Coordonner l'ensemble du processus de scrapping
- **ResponsabilitÃ©s** :
  - Orchestration des appels aux services
  - Gestion des processus d'import
  - Monitoring et mÃ©triques
  - Interface unifiÃ©e pour le reste du projet
- **CaractÃ©ristiques** : **Point d'entrÃ©e unique** pour toutes les opÃ©rations de scrapping

## ğŸ”„ Flux de donnÃ©es

```
Site externe (DofusDB) 
    â†“
DataCollect (rÃ©cupÃ©ration des donnÃ©es brutes)
    â†“
DataConversion (conversion selon caractÃ©ristiques KrosmozJDR)
    â†“
DataIntegration (mapping et sauvegarde)
    â†“
Base de donnÃ©es KrosmozJDR
```

## ğŸ“ Structure des dossiers

```
app/Services/Scrapping/
â”œâ”€â”€ DataCollect/           # Service de collecte
â”‚   â”œâ”€â”€ DataCollectService.php
â”‚   â””â”€â”€ config.php
â”œâ”€â”€ DataConversion/        # Service de conversion
â”‚   â”œâ”€â”€ DataConversionService.php
â”‚   â””â”€â”€ config.php
â”œâ”€â”€ DataIntegration/       # Service d'intÃ©gration
â”‚   â”œâ”€â”€ DataIntegrationService.php
â”‚   â””â”€â”€ config.php
â””â”€â”€ Orchestrator/          # Service d'orchestration
    â”œâ”€â”€ ScrappingOrchestrator.php
    â””â”€â”€ config.php

config/
â”œâ”€â”€ scrapping.php          # Configuration globale
â””â”€â”€ characteristics.php    # CaractÃ©ristiques du jeu

docs/50-FonctionnalitÃ©s/Scrapping/
â”œâ”€â”€ README.md              # Ce fichier
â”œâ”€â”€ Data-collect/          # Documentation DataCollect
â”œâ”€â”€ Data-conversion/       # Documentation DataConversion
â”œâ”€â”€ Data-integration/      # Documentation DataIntegration
â””â”€â”€ Orchestrator/          # Documentation Orchestrator
```

## ğŸŒ Sources de donnÃ©es supportÃ©es

### **DofusDB (Source principale)**
- **URL** : `https://api.dofusdb.fr`
- **Format** : JSON REST API
- **Langues** : fr, en, de, es, pt
- **EntitÃ©s** : Classes, monstres, objets, sorts, effets, ensembles
- **Rate limiting** : 60 requÃªtes/minute

### **ExtensibilitÃ©**
L'architecture permet d'ajouter facilement d'autres sources de donnÃ©es en crÃ©ant de nouveaux services de collecte.

## ğŸ”§ Configuration

### **Fichiers de configuration**
- **`config/scrapping.php`** : Configuration globale de tous les services
- **`config/characteristics.php`** : CaractÃ©ristiques du jeu et formules
- **`app/Services/Scrapping/*/config.php`** : Configuration spÃ©cifique Ã  chaque service

### **Variables d'environnement principales**
```bash
# Activation du scrapping
SCRAPPING_ENABLED=true

# Configuration DofusDB
DOFUSDB_BASE_URL=https://api.dofusdb.fr
DOFUSDB_DEFAULT_LANGUAGE=fr

# Limites et timeouts
SCRAPPING_MAX_CONCURRENT=3
SCRAPPING_PROCESS_TIMEOUT=3600
SCRAPPING_COLLECT_TIMEOUT=30

# Cache et rate limiting
SCRAPPING_CACHE_ENABLED=true
SCRAPPING_RATE_LIMITING_ENABLED=true
SCRAPPING_RATE_LIMIT_REQUESTS=60
```

## ğŸ“Š EntitÃ©s et mapping

### **Mapping DofusDB â†’ KrosmozJDR**
- **`breeds`** â†’ **`classes`** (Classes de personnages)
- **`monsters`** â†’ **`monsters` + `creatures`** (Monstres et crÃ©atures)
- **`items`** â†’ **`consumables`**, **`resources`**, **`items`** (selon type/catÃ©gorie)
- **`Ensemble d'items`** â†’ **`panoplies`** (Ensembles d'Ã©quipements)
- **`spells` + `spell-levels`** â†’ **`spells`** (Sorts avec niveaux)
- **`effects`** â†’ **`effects`** (Effets et bonus)

### **Types d'objets supportÃ©s**
- **Armes** (typeId=1) : Ã‰pÃ©es, haches, bÃ¢tons, etc.
- **Armures** (typeId=2) : Plastrons, jambiÃ¨res, etc.
- **Boucliers** (typeId=3) : Boucliers de dÃ©fense
- **Anneaux** (typeId=9) : Anneaux magiques
- **Amulettes** (typeId=10) : Amulettes de protection
- **Ceintures** (typeId=11) : Ceintures de force
- **Potions** (typeId=12) : Potions de soin, buffs
- **Bottes** (typeId=13) : Bottes de vitesse
- **Chapeaux** (typeId=14) : Chapeaux magiques
- **Ressources** (typeId=15) : MatÃ©riaux de craft
- **Ã‰quipements** (typeId=16) : Ã‰quipements divers
- **Fleurs** (typeId=35) : Fleurs et plantes

## ğŸš€ Utilisation

### ğŸ–¥ï¸ Interface d'administration (Vue 3)

Une interface dÃ©diÃ©e est disponible pour les administrateurs (`/scrapping`, route `scrapping.index`). Elle est responsive (mobile â†’ desktop) et propose quatre onglets :

- **EntitÃ©** : import unitaire avec formulaires dâ€™options (skip cache, force update, dry-run, validation). Un bouton *PrÃ©visualiser* lance un fetch `GET /api/scrapping/preview/{type}/{id}` afin dâ€™afficher :
  - Les donnÃ©es brutes converties.
  - Lâ€™Ã©ventuelle version dÃ©jÃ  prÃ©sente en base.
  - Un tableau de diff (champ, valeur actuelle, valeur importÃ©e) pour dÃ©cider de conserver ou dâ€™Ã©craser lâ€™entrÃ©e.
- **Plage dâ€™ID** : import dâ€™un intervalle (`start_id`, `end_id`). Le formulaire calcule le nombre dâ€™entitÃ©s concernÃ©es et vÃ©rifie la limite autorisÃ©e (classes 1â€‘19, monstres 1â€‘5000, etc.). Le bouton envoie `POST /api/scrapping/import/range`.
- **Import complet** : exÃ©cute `POST /api/scrapping/import/all` pour scrapper tout un type dâ€™un coup (utile aprÃ¨s un wipe). Un `Alert` rappelle que lâ€™opÃ©ration est longue.
- **RÃ©sultats** : historique horodatÃ© de toutes les actions (entitÃ©, plage, import complet). Chaque entrÃ©e conserve la rÃ©ponse JSON et les erreurs Ã©ventuelles pour audit.

Chaque action enregistre son rÃ©sultat localement (pas besoin de recharger) et bascule automatiquement sur lâ€™onglet *RÃ©sultats*. Le panneau de prÃ©visualisation reste disponible tant quâ€™on ne le ferme pas ou quâ€™on nâ€™importe pas la nouvelle version.

### **Via l'Orchestrateur (RecommandÃ©)**
```php
use App\Services\Scrapping\Orchestrator\ScrappingOrchestrator;

$orchestrator = app(ScrappingOrchestrator::class);

// Import d'une classe
$result = $orchestrator->importClass(1);

// Import d'un monstre
$result = $orchestrator->importMonster(100);

// Import d'un objet
$result = $orchestrator->importItem(1000);

// Import en lot
$result = $orchestrator->importBatch([
    ['type' => 'class', 'id' => 1],
    ['type' => 'monster', 'id' => 100],
]);
```

### **Via les services individuels**
```php
use App\Services\Scrapping\DataCollect\DataCollectService;
use App\Services\Scrapping\DataConversion\DataConversionService;
use App\Services\Scrapping\DataIntegration\DataIntegrationService;

$collectService = app(DataCollectService::class);
$conversionService = app(DataConversionService::class);
$integrationService = app(DataIntegrationService::class);

// Collecte
$rawData = $collectService->collectClass(1);

// Conversion
$convertedData = $conversionService->convertClass($rawData);

// IntÃ©gration
$result = $integrationService->integrateClass($convertedData);
```

### ğŸ–¼ï¸ Backfill des images locales (entitÃ©s dÃ©jÃ  importÃ©es)

Quand des entitÃ©s existent dÃ©jÃ  en base avec une image distante (ou sans image), vous pouvez **tÃ©lÃ©charger et stocker localement** les images DofusDB, sans relancer un scrapping complet.

Variables utiles (voir `config/scrapping.php`) :

```bash
# Active/dÃ©sactive le tÃ©lÃ©chargement et stockage des images
SCRAPPING_IMAGES_ENABLED=true

# RÃ©pertoire et disk (Laravel)
SCRAPPING_IMAGES_DISK=public
SCRAPPING_IMAGES_BASE_DIR="scrapping/images"

# Limite de sÃ©curitÃ©
SCRAPPING_IMAGES_MAX_BYTES=5242880
SCRAPPING_IMAGES_TIMEOUT=15
```

Commandes :

```bash
# PrÃ©visualisation (ne tÃ©lÃ©charge rien, n'Ã©crit rien)
php artisan scrapping:backfill-images --limit=50 --dry-run

# Backfill sur toutes les entitÃ©s (resources/items/consumables/spells/monsters)
php artisan scrapping:backfill-images --limit=500

# Backfill ciblÃ©
php artisan scrapping:backfill-images resource --limit=200

# Re-tÃ©lÃ©charge mÃªme si l'image locale existe dÃ©jÃ 
php artisan scrapping:backfill-images resource --force --limit=200
```

### ğŸ“Š Tables â€œhybridesâ€ (serveur + client) avec TanStack Table

Certaines pages dâ€™administration utilisent une table centralisÃ©e (`EntityTable.vue`) capable de fonctionner en **2 modes** :

- **Mode serveur** (par dÃ©faut) : la pagination/filtrage/tri passe par Inertia + backend (stable pour trÃ¨s gros volumes).
- **Mode client** : on charge un lot important via API **Ã  partir des filtres serveur courants** (baseline), puis **tri/filtre/recherche/pagination** se font instantanÃ©ment cÃ´tÃ© navigateur (TanStack Table), avec **export CSV**. Les filtres UI deviennent alors une **couche additionnelle client** (ils ne peuvent pas Ã©largir au-delÃ  du sous-ensemble chargÃ©).

Endpoints utilisÃ©s (chargement â€œmode clientâ€) :

- `GET /api/entity-table/resources?limit=5000`
- `GET /api/entity-table/resource-types?limit=5000`

Notes :
- Le `limit` est **bornÃ©** cÃ´tÃ© backend (par dÃ©faut 5000, max 20000) pour Ã©viter les charges excessives.
- Pour de trÃ¨s gros volumes, gardez le **mode serveur** et utilisez le mode client sur des lots ciblÃ©s (ex: aprÃ¨s un filtre serveur).

## ğŸ“ˆ Monitoring et mÃ©triques

### **MÃ©triques collectÃ©es**
- **Taux de succÃ¨s** des conversions et intÃ©grations
- **Temps de traitement** par type d'entitÃ©
- **Taux d'erreurs** et corrections automatiques
- **Utilisation des ressources** (mÃ©moire, CPU)
- **Performance du cache** et des API externes

### **Seuils d'alerte**
- **Taux d'erreur** > 10% â†’ Alerte
- **Temps de traitement** > 5 minutes â†’ Alerte
- **Utilisation mÃ©moire** > 80% â†’ Alerte
- **Taux de succÃ¨s** < 95% â†’ Alerte

## ğŸ”’ SÃ©curitÃ©

### **Mesures de sÃ©curitÃ©**
- **Validation des entrÃ©es** utilisateur
- **Sanitisation des donnÃ©es** collectÃ©es
- **Rate limiting** pour Ã©viter la surcharge
- **Logs d'audit** pour tracer les actions
- **Gestion des erreurs** sans exposition d'informations sensibles

### **Permissions requises**
- **Lecture** des donnÃ©es externes
- **Ã‰criture** dans la base KrosmozJDR
- **ExÃ©cution** des commandes Artisan
- **AccÃ¨s** aux logs et mÃ©triques

## ğŸ§ª Tests

### **Types de tests**
- **Tests unitaires** : Chaque service individuellement
- **Tests d'intÃ©gration** : Communication entre services
- **Tests end-to-end** : Workflow complet de scrapping
- **Tests de performance** : Charge et limites

### **ExÃ©cution des tests**
```bash
# Tests unitaires
php artisan test --filter=Scrapping

# Tests avec couverture
php artisan test --coverage --filter=Scrapping

# Tests de performance
php artisan test --filter=ScrappingPerformance
```

## ğŸ“š Documentation

### **Structure de la documentation**
Chaque composant possÃ¨de sa propre documentation dans `docs/50-FonctionnalitÃ©s/Scrapping/` :

- **`README.md`** : Vue d'ensemble et utilisation
- **`DEFINITIONS.md`** : DÃ©finitions des donnÃ©es et structures
- **`SPECIFICATIONS.md`** : Cahier des charges dÃ©taillÃ©
- **`API.md`** : Documentation de l'API et des endpoints

### **Documentation technique**
- **Architecture** : SchÃ©mas et flux de donnÃ©es
- **Configuration** : Variables d'environnement et paramÃ¨tres
- **DÃ©pannage** : Solutions aux problÃ¨mes courants
- **FAQ** : Questions frÃ©quemment posÃ©es

## ğŸ”„ Maintenance

### **TÃ¢ches de maintenance**
- **Nettoyage du cache** : Suppression des donnÃ©es expirÃ©es
- **Rotation des logs** : Archivage des anciens logs
- **Mise Ã  jour des mÃ©triques** : Nettoyage des mÃ©triques obsolÃ¨tes
- **VÃ©rification de la santÃ©** : Monitoring des services externes

### **Commandes de maintenance**
```bash
# Nettoyage du cache
php artisan scrapping:clear-cache

# VÃ©rification de la santÃ©
php artisan scrapping:health-check

# Nettoyage des logs
php artisan scrapping:cleanup-logs

# Mise Ã  jour des mÃ©triques
php artisan scrapping:update-metrics
```

## ğŸš¨ DÃ©pannage

### **ProblÃ¨mes courants**
1. **API DofusDB inaccessible** : VÃ©rifier la connectivitÃ© rÃ©seau
2. **Rate limiting dÃ©passÃ©** : RÃ©duire la frÃ©quence des requÃªtes
3. **Erreurs de conversion** : VÃ©rifier la configuration des caractÃ©ristiques
4. **Ã‰checs d'intÃ©gration** : VÃ©rifier la structure de la base de donnÃ©es

### **Logs et debugging**
- **Logs dÃ©taillÃ©s** dans `storage/logs/scrapping.log`
- **MÃ©triques en temps rÃ©el** via l'API de monitoring
- **Mode debug** activable via configuration
- **TraÃ§abilitÃ©** des processus avec IDs de corrÃ©lation

## ğŸ”® Ã‰volutions futures

### **FonctionnalitÃ©s prÃ©vues**
- **Support multi-sources** : IntÃ©gration d'autres sites de donnÃ©es
- **Synchronisation automatique** : Mise Ã  jour pÃ©riodique des donnÃ©es
- **Interface web** : Dashboard de monitoring et contrÃ´le
- **Webhooks** : Notifications en temps rÃ©el
- **Export des donnÃ©es** : Formats multiples (JSON, CSV, XML)

### **AmÃ©liorations techniques**
- **Cache distribuÃ©** : Support Redis/Memcached
- **Traitement asynchrone** : Queues et jobs en arriÃ¨re-plan
- **API GraphQL** : Interface de requÃªte plus flexible
- **Microservices** : DÃ©composition en services indÃ©pendants

---

## ğŸ“ Support

Pour toute question ou problÃ¨me avec cette fonctionnalitÃ© :

1. **Consulter la documentation** de chaque composant
2. **VÃ©rifier les logs** pour identifier les erreurs
3. **Consulter les mÃ©triques** pour diagnostiquer les problÃ¨mes
4. **Contacter l'Ã©quipe de dÃ©veloppement** si nÃ©cessaire

---

*DerniÃ¨re mise Ã  jour : DÃ©cembre 2024*